{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandt555/BigData-Practice/blob/main/cifar10c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7p3X9FRdsv2",
        "outputId": "41c7b129-d694-496c-e0a8-4d25e5c067c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-09 16:38:56--  https://zenodo.org/record/2535967/files/CIFAR-10-C.tar?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/2535967/files/CIFAR-10-C.tar [following]\n",
            "--2024-07-09 16:38:56--  https://zenodo.org/records/2535967/files/CIFAR-10-C.tar\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2918471680 (2.7G) [application/octet-stream]\n",
            "Saving to: ‘CIFAR-10-C.tar’\n",
            "\n",
            "CIFAR-10-C.tar      100%[===================>]   2.72G  29.1MB/s    in 96s     \n",
            "\n",
            "2024-07-09 16:40:33 (28.9 MB/s) - ‘CIFAR-10-C.tar’ saved [2918471680/2918471680]\n",
            "\n",
            "CIFAR-10-C/\n",
            "CIFAR-10-C/fog.npy\n",
            "CIFAR-10-C/jpeg_compression.npy\n",
            "CIFAR-10-C/zoom_blur.npy\n",
            "CIFAR-10-C/speckle_noise.npy\n",
            "CIFAR-10-C/glass_blur.npy\n",
            "CIFAR-10-C/spatter.npy\n",
            "CIFAR-10-C/shot_noise.npy\n",
            "CIFAR-10-C/defocus_blur.npy\n",
            "CIFAR-10-C/elastic_transform.npy\n",
            "CIFAR-10-C/gaussian_blur.npy\n",
            "CIFAR-10-C/frost.npy\n",
            "CIFAR-10-C/saturate.npy\n",
            "CIFAR-10-C/brightness.npy\n",
            "CIFAR-10-C/snow.npy\n",
            "CIFAR-10-C/gaussian_noise.npy\n",
            "CIFAR-10-C/motion_blur.npy\n",
            "CIFAR-10-C/contrast.npy\n",
            "CIFAR-10-C/impulse_noise.npy\n",
            "CIFAR-10-C/labels.npy\n",
            "CIFAR-10-C/pixelate.npy\n",
            "CIFAR-10-C\n"
          ]
        }
      ],
      "source": [
        "# Download and extract CIFAR-10C dataset\n",
        "!wget -O CIFAR-10-C.tar https://zenodo.org/record/2535967/files/CIFAR-10-C.tar?download=1\n",
        "!mkdir -p ./data/CIFAR-10-C\n",
        "!tar -xvf CIFAR-10-C.tar -C ./data/CIFAR-10-C\n",
        "\n",
        "# Verify extraction\n",
        "!ls ./data/CIFAR-10-C"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evidential Deep Learning with Custom Loss - CIFAR-10C (Gaussian Noise)"
      ],
      "metadata": {
        "id": "yDLLcTnL6T4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqESYsiIRNJ0",
        "outputId": "b91fc15c-279c-41f5-af99-b7031b51d020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "epoch 1 - training accuracy: 0.6120 \t training uncertainty: 0.4598 \t testing accuracy: 0.5635 \t testing uncertainty: 0.4690\n",
            "epoch 2 - training accuracy: 0.6405 \t training uncertainty: 0.3663 \t testing accuracy: 0.6074 \t testing uncertainty: 0.4061\n",
            "epoch 3 - training accuracy: 0.7196 \t training uncertainty: 0.4099 \t testing accuracy: 0.6195 \t testing uncertainty: 0.4013\n",
            "epoch 4 - training accuracy: 0.7507 \t training uncertainty: 0.3117 \t testing accuracy: 0.6315 \t testing uncertainty: 0.3089\n",
            "epoch 5 - training accuracy: 0.7918 \t training uncertainty: 0.3082 \t testing accuracy: 0.6649 \t testing uncertainty: 0.3145\n",
            "epoch 6 - training accuracy: 0.8056 \t training uncertainty: 0.1925 \t testing accuracy: 0.6578 \t testing uncertainty: 0.3032\n",
            "epoch 7 - training accuracy: 0.8225 \t training uncertainty: 0.2516 \t testing accuracy: 0.6657 \t testing uncertainty: 0.2870\n",
            "epoch 8 - training accuracy: 0.8384 \t training uncertainty: 0.3509 \t testing accuracy: 0.6721 \t testing uncertainty: 0.2587\n",
            "epoch 9 - training accuracy: 0.8522 \t training uncertainty: 0.3965 \t testing accuracy: 0.6759 \t testing uncertainty: 0.2239\n",
            "epoch 10 - training accuracy: 0.8582 \t training uncertainty: 0.3268 \t testing accuracy: 0.6678 \t testing uncertainty: 0.2494\n",
            "epoch 11 - training accuracy: 0.8651 \t training uncertainty: 0.3257 \t testing accuracy: 0.6583 \t testing uncertainty: 0.2830\n",
            "epoch 12 - training accuracy: 0.8726 \t training uncertainty: 0.1658 \t testing accuracy: 0.6724 \t testing uncertainty: 0.2250\n",
            "epoch 13 - training accuracy: 0.8712 \t training uncertainty: 0.1717 \t testing accuracy: 0.6602 \t testing uncertainty: 0.2411\n",
            "epoch 14 - training accuracy: 0.8797 \t training uncertainty: 0.0946 \t testing accuracy: 0.6661 \t testing uncertainty: 0.2026\n",
            "epoch 15 - training accuracy: 0.8723 \t training uncertainty: 0.2233 \t testing accuracy: 0.6471 \t testing uncertainty: 0.2200\n",
            "epoch 16 - training accuracy: 0.8830 \t training uncertainty: 0.1268 \t testing accuracy: 0.6689 \t testing uncertainty: 0.1481\n",
            "epoch 17 - training accuracy: 0.8854 \t training uncertainty: 0.1171 \t testing accuracy: 0.6607 \t testing uncertainty: 0.1494\n",
            "epoch 18 - training accuracy: 0.8833 \t training uncertainty: 0.1887 \t testing accuracy: 0.6479 \t testing uncertainty: 0.2088\n",
            "epoch 19 - training accuracy: 0.8886 \t training uncertainty: 0.0644 \t testing accuracy: 0.6534 \t testing uncertainty: 0.1838\n",
            "epoch 20 - training accuracy: 0.8886 \t training uncertainty: 0.2425 \t testing accuracy: 0.6470 \t testing uncertainty: 0.2218\n",
            "epoch 21 - training accuracy: 0.8869 \t training uncertainty: 0.2652 \t testing accuracy: 0.6325 \t testing uncertainty: 0.2557\n",
            "epoch 22 - training accuracy: 0.8923 \t training uncertainty: 0.1938 \t testing accuracy: 0.6510 \t testing uncertainty: 0.1873\n",
            "epoch 23 - training accuracy: 0.8920 \t training uncertainty: 0.1211 \t testing accuracy: 0.6499 \t testing uncertainty: 0.2395\n",
            "epoch 24 - training accuracy: 0.8954 \t training uncertainty: 0.1251 \t testing accuracy: 0.6616 \t testing uncertainty: 0.1352\n",
            "epoch 25 - training accuracy: 0.8938 \t training uncertainty: 0.0953 \t testing accuracy: 0.6424 \t testing uncertainty: 0.2796\n",
            "epoch 26 - training accuracy: 0.8935 \t training uncertainty: 0.1443 \t testing accuracy: 0.6487 \t testing uncertainty: 0.2257\n",
            "epoch 27 - training accuracy: 0.8877 \t training uncertainty: 0.0724 \t testing accuracy: 0.6420 \t testing uncertainty: 0.1661\n",
            "epoch 28 - training accuracy: 0.8900 \t training uncertainty: 0.0982 \t testing accuracy: 0.6397 \t testing uncertainty: 0.2448\n",
            "epoch 29 - training accuracy: 0.8925 \t training uncertainty: 0.1024 \t testing accuracy: 0.6398 \t testing uncertainty: 0.1353\n",
            "epoch 30 - training accuracy: 0.8969 \t training uncertainty: 0.0252 \t testing accuracy: 0.6523 \t testing uncertainty: 0.1859\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Download and extract CIFAR-10C dataset\n",
        "# !wget -O CIFAR-10-C.tar https://zenodo.org/record/2535967/files/CIFAR-10-C.tar?download=1\n",
        "# !mkdir -p ./data/CIFAR-10-C\n",
        "# !tar -xvf CIFAR-10-C.tar -C ./data/CIFAR-10-C\n",
        "\n",
        "# # Verify extraction\n",
        "# !ls ./data/CIFAR-10-C\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download CIFAR-10 dataset for training\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define CIFAR-10C dataset class for testing\n",
        "class CIFAR10C(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, corruption_type='gaussian_noise', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.corruption_type = corruption_type\n",
        "        self.data_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', f'{corruption_type}.npy')\n",
        "        self.labels_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', 'labels.npy')\n",
        "\n",
        "        self.data = np.load(self.data_path)\n",
        "        self.labels = np.load(self.labels_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.labels[idx]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Define transformation for CIFAR-10C\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10C dataset for testing\n",
        "test_dataset_cifar10c = CIFAR10C(root='./data', corruption_type='gaussian_noise', transform=test_transform)\n",
        "test_loader_cifar10c = DataLoader(test_dataset_cifar10c, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichletCIFAR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFAR, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = F.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Initialize model, optimizer\n",
        "lenet_dirichlet_CIFAR = LeNetDirichletCIFAR()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    lenet_dirichlet_CIFAR.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    # Evaluation on training set\n",
        "    lenet_dirichlet_CIFAR.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    # Evaluation on CIFAR-10C (testing set)\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_cifar10c:\n",
        "            outputs, uncertainty_test, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print metrics\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evidential Deep Learning + Mixup with Custom Loss - CIFAR-10C (Gaussian Noise)"
      ],
      "metadata": {
        "id": "4ok5P1Vu7cno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7u4kPRpRP3m",
        "outputId": "993e986a-ce97-4430-b76a-1566798a08df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 1 - training accuracy: 0.5806 \t training uncertainty: 0.4947 \t testing accuracy on CIFAR-10C: 0.5613 \t testing uncertainty on CIFAR-10C: 0.4676\n",
            "epoch 2 - training accuracy: 0.6825 \t training uncertainty: 0.3492 \t testing accuracy on CIFAR-10C: 0.6192 \t testing uncertainty on CIFAR-10C: 0.3683\n",
            "epoch 3 - training accuracy: 0.7205 \t training uncertainty: 0.3969 \t testing accuracy on CIFAR-10C: 0.6288 \t testing uncertainty on CIFAR-10C: 0.3763\n",
            "epoch 4 - training accuracy: 0.7400 \t training uncertainty: 0.3862 \t testing accuracy on CIFAR-10C: 0.6273 \t testing uncertainty on CIFAR-10C: 0.3217\n",
            "epoch 5 - training accuracy: 0.7795 \t training uncertainty: 0.3534 \t testing accuracy on CIFAR-10C: 0.6655 \t testing uncertainty on CIFAR-10C: 0.3249\n",
            "epoch 6 - training accuracy: 0.7931 \t training uncertainty: 0.3866 \t testing accuracy on CIFAR-10C: 0.6623 \t testing uncertainty on CIFAR-10C: 0.3173\n",
            "epoch 7 - training accuracy: 0.8100 \t training uncertainty: 0.3459 \t testing accuracy on CIFAR-10C: 0.6800 \t testing uncertainty on CIFAR-10C: 0.2764\n",
            "epoch 8 - training accuracy: 0.8218 \t training uncertainty: 0.2611 \t testing accuracy on CIFAR-10C: 0.6700 \t testing uncertainty on CIFAR-10C: 0.2836\n",
            "epoch 9 - training accuracy: 0.8411 \t training uncertainty: 0.4589 \t testing accuracy on CIFAR-10C: 0.6717 \t testing uncertainty on CIFAR-10C: 0.3019\n",
            "epoch 10 - training accuracy: 0.8487 \t training uncertainty: 0.2568 \t testing accuracy on CIFAR-10C: 0.6818 \t testing uncertainty on CIFAR-10C: 0.2660\n",
            "epoch 11 - training accuracy: 0.8490 \t training uncertainty: 0.1700 \t testing accuracy on CIFAR-10C: 0.6754 \t testing uncertainty on CIFAR-10C: 0.2647\n",
            "epoch 12 - training accuracy: 0.8641 \t training uncertainty: 0.2624 \t testing accuracy on CIFAR-10C: 0.6768 \t testing uncertainty on CIFAR-10C: 0.2610\n",
            "epoch 13 - training accuracy: 0.8649 \t training uncertainty: 0.1732 \t testing accuracy on CIFAR-10C: 0.6764 \t testing uncertainty on CIFAR-10C: 0.1513\n",
            "epoch 14 - training accuracy: 0.8732 \t training uncertainty: 0.0845 \t testing accuracy on CIFAR-10C: 0.6714 \t testing uncertainty on CIFAR-10C: 0.2267\n",
            "epoch 15 - training accuracy: 0.8702 \t training uncertainty: 0.0906 \t testing accuracy on CIFAR-10C: 0.6705 \t testing uncertainty on CIFAR-10C: 0.2495\n",
            "epoch 16 - training accuracy: 0.8714 \t training uncertainty: 0.0816 \t testing accuracy on CIFAR-10C: 0.6643 \t testing uncertainty on CIFAR-10C: 0.2735\n",
            "epoch 17 - training accuracy: 0.8744 \t training uncertainty: 0.2316 \t testing accuracy on CIFAR-10C: 0.6696 \t testing uncertainty on CIFAR-10C: 0.2627\n",
            "epoch 18 - training accuracy: 0.8828 \t training uncertainty: 0.0690 \t testing accuracy on CIFAR-10C: 0.6734 \t testing uncertainty on CIFAR-10C: 0.2225\n",
            "epoch 19 - training accuracy: 0.8812 \t training uncertainty: 0.1799 \t testing accuracy on CIFAR-10C: 0.6600 \t testing uncertainty on CIFAR-10C: 0.1779\n",
            "epoch 20 - training accuracy: 0.8821 \t training uncertainty: 0.1809 \t testing accuracy on CIFAR-10C: 0.6582 \t testing uncertainty on CIFAR-10C: 0.1704\n",
            "epoch 21 - training accuracy: 0.8769 \t training uncertainty: 0.2559 \t testing accuracy on CIFAR-10C: 0.6585 \t testing uncertainty on CIFAR-10C: 0.1932\n",
            "epoch 22 - training accuracy: 0.8837 \t training uncertainty: 0.2211 \t testing accuracy on CIFAR-10C: 0.6593 \t testing uncertainty on CIFAR-10C: 0.1353\n",
            "epoch 23 - training accuracy: 0.8875 \t training uncertainty: 0.2293 \t testing accuracy on CIFAR-10C: 0.6692 \t testing uncertainty on CIFAR-10C: 0.1584\n",
            "epoch 24 - training accuracy: 0.8852 \t training uncertainty: 0.2064 \t testing accuracy on CIFAR-10C: 0.6514 \t testing uncertainty on CIFAR-10C: 0.3184\n",
            "epoch 25 - training accuracy: 0.8862 \t training uncertainty: 0.1823 \t testing accuracy on CIFAR-10C: 0.6537 \t testing uncertainty on CIFAR-10C: 0.1927\n",
            "epoch 26 - training accuracy: 0.8903 \t training uncertainty: 0.2220 \t testing accuracy on CIFAR-10C: 0.6579 \t testing uncertainty on CIFAR-10C: 0.1487\n",
            "epoch 27 - training accuracy: 0.8910 \t training uncertainty: 0.0361 \t testing accuracy on CIFAR-10C: 0.6723 \t testing uncertainty on CIFAR-10C: 0.0739\n",
            "epoch 28 - training accuracy: 0.8889 \t training uncertainty: 0.1211 \t testing accuracy on CIFAR-10C: 0.6541 \t testing uncertainty on CIFAR-10C: 0.1637\n",
            "epoch 29 - training accuracy: 0.8924 \t training uncertainty: 0.1134 \t testing accuracy on CIFAR-10C: 0.6713 \t testing uncertainty on CIFAR-10C: 0.0971\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader_cifar10 = DataLoader(dataset=test_dataset_cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define CIFAR-10C dataset class for testing\n",
        "class CIFAR10C(Dataset):\n",
        "    def __init__(self, root, corruption_type='gaussian_noise', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.corruption_type = corruption_type\n",
        "        self.data_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', f'{corruption_type}.npy')\n",
        "        self.labels_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', 'labels.npy')\n",
        "\n",
        "        self.data = np.load(self.data_path)\n",
        "        self.labels = np.load(self.labels_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.labels[idx]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Define transformation for CIFAR-10C\n",
        "test_transform_cifar10c = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10C dataset for testing\n",
        "test_dataset_cifar10c = CIFAR10C(root='./data', corruption_type='gaussian_noise', transform=test_transform_cifar10c)\n",
        "test_loader_cifar10c = DataLoader(test_dataset_cifar10c, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichletCIFARMixup(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFARMixup, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Updated mixup function for Evidential Deep Learning\n",
        "def mixup_data(x, y, alpha=0.1):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_onehot = torch.zeros(y.size(0), 10).to(x.device)\n",
        "    y_onehot.scatter_(1, y.view(-1, 1).long(), 1)\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "    mixed_y = mixed_y.argmax(dim=1)\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = F.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution and mixup for Evidential Deep Learning\n",
        "lenet_dirichlet_CIFAR_mixup = LeNetDirichletCIFARMixup()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR_mixup.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(30):\n",
        "    lenet_dirichlet_CIFAR_mixup.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = mixup_data(inputs, labels.unsqueeze(1))  # Assuming labels are one-dimensional\n",
        "\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "\n",
        "        # Cast labels back to Long data type\n",
        "        labels = labels.squeeze().long()\n",
        "\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet_CIFAR_mixup.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    correct_test_cifar10c = 0\n",
        "    total_test_cifar10c = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_cifar10c:\n",
        "            outputs, uncertainty_test_cifar10c , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test_cifar10c += labels.size(0)\n",
        "            correct_test_cifar10c += (predicted == labels).sum().item()\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy on CIFAR-10C: %2.4f \\t testing uncertainty on CIFAR-10C: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test_cifar10c / total_test_cifar10c, uncertainty_test_cifar10c.mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbM2YbE5bWOt",
        "outputId": "fea9e5df-45f7-48d3-e355-bb38c7e6647a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "epoch 1 - training accuracy: 0.6265 \t training uncertainty: 0.5575 \t testing accuracy: 0.5563 \t testing uncertainty: 0.5760\n",
            "epoch 2 - training accuracy: 0.6939 \t training uncertainty: 0.4426 \t testing accuracy: 0.5510 \t testing uncertainty: 0.4492\n",
            "epoch 3 - training accuracy: 0.7472 \t training uncertainty: 0.4744 \t testing accuracy: 0.5896 \t testing uncertainty: 0.4292\n",
            "epoch 4 - training accuracy: 0.7783 \t training uncertainty: 0.5037 \t testing accuracy: 0.5777 \t testing uncertainty: 0.3341\n",
            "epoch 5 - training accuracy: 0.8083 \t training uncertainty: 0.2774 \t testing accuracy: 0.6119 \t testing uncertainty: 0.4560\n",
            "epoch 6 - training accuracy: 0.8291 \t training uncertainty: 0.3770 \t testing accuracy: 0.5927 \t testing uncertainty: 0.3859\n",
            "epoch 7 - training accuracy: 0.8531 \t training uncertainty: 0.2225 \t testing accuracy: 0.5914 \t testing uncertainty: 0.3551\n",
            "epoch 8 - training accuracy: 0.8598 \t training uncertainty: 0.3122 \t testing accuracy: 0.5860 \t testing uncertainty: 0.3965\n",
            "epoch 9 - training accuracy: 0.8745 \t training uncertainty: 0.1434 \t testing accuracy: 0.5949 \t testing uncertainty: 0.3415\n",
            "epoch 10 - training accuracy: 0.8771 \t training uncertainty: 0.1679 \t testing accuracy: 0.5966 \t testing uncertainty: 0.3083\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Download and extract CIFAR-10C dataset\n",
        "# !wget -O CIFAR-10-C.tar https://zenodo.org/record/2535967/files/CIFAR-10-C.tar?download=1\n",
        "# !mkdir -p ./data/CIFAR-10-C\n",
        "# !tar -xvf CIFAR-10-C.tar -C ./data/CIFAR-10-C\n",
        "\n",
        "# # Verify extraction\n",
        "# !ls ./data/CIFAR-10-C\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download CIFAR-10 dataset for training\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define CIFAR-10C dataset class for testing\n",
        "class CIFAR10C(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, corruption_type='motion_blur', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.corruption_type = corruption_type\n",
        "        self.data_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', f'{corruption_type}.npy')\n",
        "        self.labels_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', 'labels.npy')\n",
        "\n",
        "        self.data = np.load(self.data_path)\n",
        "        self.labels = np.load(self.labels_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.labels[idx]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Define transformation for CIFAR-10C\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10C dataset for testing\n",
        "test_dataset_cifar10c = CIFAR10C(root='./data', corruption_type='motion_blur', transform=test_transform)\n",
        "test_loader_cifar10c = DataLoader(test_dataset_cifar10c, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichletCIFAR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFAR, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = F.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Initialize model, optimizer\n",
        "lenet_dirichlet_CIFAR = LeNetDirichletCIFAR()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet_CIFAR.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    # Evaluation on training set\n",
        "    lenet_dirichlet_CIFAR.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    # Evaluation on CIFAR-10C (testing set)\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_cifar10c:\n",
        "            outputs, uncertainty_test, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print metrics\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGJpJ7MsqrS0",
        "outputId": "7942f37c-f294-4e99-ca05-c538960c49c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 1 - training accuracy: 0.5787 \t training uncertainty: 0.5134 \t testing accuracy on CIFAR-10C: 0.5148 \t testing uncertainty on CIFAR-10C: 0.5457\n",
            "epoch 2 - training accuracy: 0.6746 \t training uncertainty: 0.4801 \t testing accuracy on CIFAR-10C: 0.5402 \t testing uncertainty on CIFAR-10C: 0.4881\n",
            "epoch 3 - training accuracy: 0.7284 \t training uncertainty: 0.4249 \t testing accuracy on CIFAR-10C: 0.5933 \t testing uncertainty on CIFAR-10C: 0.4533\n",
            "epoch 4 - training accuracy: 0.7695 \t training uncertainty: 0.3467 \t testing accuracy on CIFAR-10C: 0.6068 \t testing uncertainty on CIFAR-10C: 0.4417\n",
            "epoch 5 - training accuracy: 0.7873 \t training uncertainty: 0.1863 \t testing accuracy on CIFAR-10C: 0.5929 \t testing uncertainty on CIFAR-10C: 0.4021\n",
            "epoch 6 - training accuracy: 0.8143 \t training uncertainty: 0.3521 \t testing accuracy on CIFAR-10C: 0.6271 \t testing uncertainty on CIFAR-10C: 0.3906\n",
            "epoch 7 - training accuracy: 0.8390 \t training uncertainty: 0.3653 \t testing accuracy on CIFAR-10C: 0.6215 \t testing uncertainty on CIFAR-10C: 0.4254\n",
            "epoch 8 - training accuracy: 0.8468 \t training uncertainty: 0.2812 \t testing accuracy on CIFAR-10C: 0.6151 \t testing uncertainty on CIFAR-10C: 0.4134\n",
            "epoch 9 - training accuracy: 0.8571 \t training uncertainty: 0.2324 \t testing accuracy on CIFAR-10C: 0.6136 \t testing uncertainty on CIFAR-10C: 0.4212\n",
            "epoch 10 - training accuracy: 0.8681 \t training uncertainty: 0.1988 \t testing accuracy on CIFAR-10C: 0.6170 \t testing uncertainty on CIFAR-10C: 0.4230\n",
            "epoch 11 - training accuracy: 0.8790 \t training uncertainty: 0.1630 \t testing accuracy on CIFAR-10C: 0.6099 \t testing uncertainty on CIFAR-10C: 0.3222\n",
            "epoch 12 - training accuracy: 0.8838 \t training uncertainty: 0.1647 \t testing accuracy on CIFAR-10C: 0.6008 \t testing uncertainty on CIFAR-10C: 0.3379\n",
            "epoch 13 - training accuracy: 0.8834 \t training uncertainty: 0.2722 \t testing accuracy on CIFAR-10C: 0.5944 \t testing uncertainty on CIFAR-10C: 0.2743\n",
            "epoch 14 - training accuracy: 0.8841 \t training uncertainty: 0.0869 \t testing accuracy on CIFAR-10C: 0.5870 \t testing uncertainty on CIFAR-10C: 0.3260\n",
            "epoch 15 - training accuracy: 0.8897 \t training uncertainty: 0.2681 \t testing accuracy on CIFAR-10C: 0.5829 \t testing uncertainty on CIFAR-10C: 0.3238\n",
            "epoch 16 - training accuracy: 0.8957 \t training uncertainty: 0.2415 \t testing accuracy on CIFAR-10C: 0.5993 \t testing uncertainty on CIFAR-10C: 0.2456\n",
            "epoch 17 - training accuracy: 0.8971 \t training uncertainty: 0.2280 \t testing accuracy on CIFAR-10C: 0.5985 \t testing uncertainty on CIFAR-10C: 0.2364\n",
            "epoch 18 - training accuracy: 0.8935 \t training uncertainty: 0.0639 \t testing accuracy on CIFAR-10C: 0.5823 \t testing uncertainty on CIFAR-10C: 0.2212\n",
            "epoch 19 - training accuracy: 0.8992 \t training uncertainty: 0.1621 \t testing accuracy on CIFAR-10C: 0.5830 \t testing uncertainty on CIFAR-10C: 0.2812\n",
            "epoch 20 - training accuracy: 0.8913 \t training uncertainty: 0.4268 \t testing accuracy on CIFAR-10C: 0.5683 \t testing uncertainty on CIFAR-10C: 0.2830\n",
            "epoch 21 - training accuracy: 0.8968 \t training uncertainty: 0.1662 \t testing accuracy on CIFAR-10C: 0.5700 \t testing uncertainty on CIFAR-10C: 0.2421\n",
            "epoch 22 - training accuracy: 0.8972 \t training uncertainty: 0.0525 \t testing accuracy on CIFAR-10C: 0.5728 \t testing uncertainty on CIFAR-10C: 0.2968\n",
            "epoch 23 - training accuracy: 0.8980 \t training uncertainty: 0.1620 \t testing accuracy on CIFAR-10C: 0.5995 \t testing uncertainty on CIFAR-10C: 0.2331\n",
            "epoch 24 - training accuracy: 0.9000 \t training uncertainty: 0.1688 \t testing accuracy on CIFAR-10C: 0.5804 \t testing uncertainty on CIFAR-10C: 0.2400\n",
            "epoch 25 - training accuracy: 0.8994 \t training uncertainty: 0.1968 \t testing accuracy on CIFAR-10C: 0.5655 \t testing uncertainty on CIFAR-10C: 0.2104\n",
            "epoch 26 - training accuracy: 0.9030 \t training uncertainty: 0.1531 \t testing accuracy on CIFAR-10C: 0.5886 \t testing uncertainty on CIFAR-10C: 0.2182\n",
            "epoch 27 - training accuracy: 0.9058 \t training uncertainty: 0.1043 \t testing accuracy on CIFAR-10C: 0.5699 \t testing uncertainty on CIFAR-10C: 0.2557\n",
            "epoch 28 - training accuracy: 0.9071 \t training uncertainty: 0.0978 \t testing accuracy on CIFAR-10C: 0.5879 \t testing uncertainty on CIFAR-10C: 0.1281\n",
            "epoch 29 - training accuracy: 0.9044 \t training uncertainty: 0.1030 \t testing accuracy on CIFAR-10C: 0.5662 \t testing uncertainty on CIFAR-10C: 0.3253\n",
            "epoch 30 - training accuracy: 0.9024 \t training uncertainty: 0.0860 \t testing accuracy on CIFAR-10C: 0.5516 \t testing uncertainty on CIFAR-10C: 0.3264\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader_cifar10 = DataLoader(dataset=test_dataset_cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define CIFAR-10C dataset class for testing\n",
        "class CIFAR10C(Dataset):\n",
        "    def __init__(self, root, corruption_type='motion_blur', transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.corruption_type = corruption_type\n",
        "        self.data_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', f'{corruption_type}.npy')\n",
        "        self.labels_path = os.path.join(root, 'CIFAR-10-C/CIFAR-10-C', 'labels.npy')\n",
        "\n",
        "        self.data = np.load(self.data_path)\n",
        "        self.labels = np.load(self.labels_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.labels[idx]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Define transformation for CIFAR-10C\n",
        "test_transform_cifar10c = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10C dataset for testing\n",
        "test_dataset_cifar10c = CIFAR10C(root='./data', corruption_type='motion_blur', transform=test_transform_cifar10c)\n",
        "test_loader_cifar10c = DataLoader(test_dataset_cifar10c, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichletCIFARMixup(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFARMixup, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Updated mixup function for Evidential Deep Learning\n",
        "def mixup_data(x, y, alpha=0.1):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_onehot = torch.zeros(y.size(0), 10).to(x.device)\n",
        "    y_onehot.scatter_(1, y.view(-1, 1).long(), 1)\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "    mixed_y = mixed_y.argmax(dim=1)\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = F.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution and mixup for Evidential Deep Learning\n",
        "lenet_dirichlet_CIFAR_mixup = LeNetDirichletCIFARMixup()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR_mixup.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(30):\n",
        "    lenet_dirichlet_CIFAR_mixup.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = mixup_data(inputs, labels.unsqueeze(1))  # Assuming labels are one-dimensional\n",
        "\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "\n",
        "        # Cast labels back to Long data type\n",
        "        labels = labels.squeeze().long()\n",
        "\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet_CIFAR_mixup.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    correct_test_cifar10c = 0\n",
        "    total_test_cifar10c = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_cifar10c:\n",
        "            outputs, uncertainty_test_cifar10c , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test_cifar10c += labels.size(0)\n",
        "            correct_test_cifar10c += (predicted == labels).sum().item()\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy on CIFAR-10C: %2.4f \\t testing uncertainty on CIFAR-10C: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test_cifar10c / total_test_cifar10c, uncertainty_test_cifar10c.mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data1', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data1', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning and mixup\n",
        "class LeNetDirichletCIFARMixup(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFARMixup, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = torch.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "lenet_dirichlet_CIFAR = LeNetDirichletCIFARMixup()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet_CIFAR.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "# FGSM attack\n",
        "def fgsm_attack(model, loss_fn, images, labels, epsilon):\n",
        "    images = images.clone().detach().requires_grad_(True)\n",
        "    outputs, _, _ = model(images)\n",
        "    loss = loss_fn(F.one_hot(labels, num_classes=10).float(), outputs)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    data_grad = images.grad.data\n",
        "    perturbed_image = images + epsilon * data_grad.sign()\n",
        "    return perturbed_image\n",
        "\n",
        "# Calculate accuracy and entropy for different perturbation levels\n",
        "epsilon_values = np.linspace(0, 0.3, 10)\n",
        "accuracy = []\n",
        "entropy = []\n",
        "\n",
        "for epsilon in epsilon_values:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_entropy = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            adv_inputs = fgsm_attack(lenet_dirichlet_CIFAR, custom_loss, inputs, labels, epsilon)\n",
        "            outputs, _, alpha = lenet_dirichlet_CIFAR(adv_inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total_entropy += -torch.sum(outputs * torch.log(outputs + 1e-10)).item()\n",
        "\n",
        "    accuracy.append(correct / total)\n",
        "    entropy.append(total_entropy / total)\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epsilon_values, accuracy, label='Accuracy')\n",
        "plt.xlabel('Adversarial Perturbation (ε)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Adversarial Perturbation')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epsilon_values, entropy, label='Entropy', color='orange')\n",
        "plt.xlabel('Adversarial Perturbation (ε)')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title('Entropy vs. Adversarial Perturbation')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "uNlujovboueh",
        "outputId": "7bcbaf8e-7d61-4435-ba60-851040a6ed30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5c377a48746b>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0madv_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgsm_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenet_dirichlet_CIFAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlenet_dirichlet_CIFAR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-5c377a48746b>\u001b[0m in \u001b[0;36mfgsm_attack\u001b[0;34m(model, loss_fn, images, labels, epsilon)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mdata_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mperturbed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiRG25msW7wxMdI3AFey+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}