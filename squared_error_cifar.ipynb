{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSfvGdAxEGAhoU7WWqUfeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandt555/BigData-Practice/blob/main/squared_error_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l8CBIapWOhN",
        "outputId": "79a65f4c-bed1-432c-895f-218b56aa02a5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data1/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12501750.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data1/cifar-10-python.tar.gz to ./data1\n",
            "Files already downloaded and verified\n",
            "epoch 1 - training accuracy: 0.5522 \t training uncertainty: 0.5751 \t testing accuracy: 0.5426 \t testing uncertainty: 0.4966\n",
            "epoch 2 - training accuracy: 0.6368 \t training uncertainty: 0.5114 \t testing accuracy: 0.6120 \t testing uncertainty: 0.3713\n",
            "epoch 3 - training accuracy: 0.6838 \t training uncertainty: 0.5464 \t testing accuracy: 0.6391 \t testing uncertainty: 0.3731\n",
            "epoch 4 - training accuracy: 0.7174 \t training uncertainty: 0.4857 \t testing accuracy: 0.6649 \t testing uncertainty: 0.2834\n",
            "epoch 5 - training accuracy: 0.7330 \t training uncertainty: 0.3691 \t testing accuracy: 0.6651 \t testing uncertainty: 0.2611\n",
            "epoch 6 - training accuracy: 0.7658 \t training uncertainty: 0.5296 \t testing accuracy: 0.6879 \t testing uncertainty: 0.2834\n",
            "epoch 7 - training accuracy: 0.7723 \t training uncertainty: 0.2332 \t testing accuracy: 0.6836 \t testing uncertainty: 0.2268\n",
            "epoch 8 - training accuracy: 0.7961 \t training uncertainty: 0.2901 \t testing accuracy: 0.6850 \t testing uncertainty: 0.2115\n",
            "epoch 9 - training accuracy: 0.8085 \t training uncertainty: 0.3569 \t testing accuracy: 0.6944 \t testing uncertainty: 0.2186\n",
            "epoch 10 - training accuracy: 0.8169 \t training uncertainty: 0.1421 \t testing accuracy: 0.6905 \t testing uncertainty: 0.2149\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data1', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data1', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning and mixup\n",
        "class LeNetDirichletCIFARMixup(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFARMixup, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Updated mixup function for Evidential Deep Learning\n",
        "def mixup_data(x, y, alpha=0.1):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_onehot = torch.zeros(y.size(0), 10).to(x.device)\n",
        "    y_onehot.scatter_(1, y.view(-1, 1).long(), 1)\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "    mixed_y = mixed_y.argmax(dim=1)\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = torch.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution and mixup for Evidential Deep Learning\n",
        "lenet_dirichlet_CIFAR_mixup = LeNetDirichletCIFARMixup()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR_mixup.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet_CIFAR_mixup.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = mixup_data(inputs, labels.unsqueeze(1))  # Assuming labels are one-dimensional\n",
        "\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "\n",
        "        # Cast labels back to Long data type\n",
        "        labels = labels.squeeze().long()\n",
        "\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet_CIFAR_mixup.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs, uncertainty_test , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data1', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data1', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichletCIFAR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFAR, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)  # Adjust input channels\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)  # Adjust the input size for fully connected layer\n",
        "        self.fc2 = nn.Linear(512, 10)  # Adjust output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1  # Use absolute values for simplicity, adjust as needed\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = torch.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "lenet_dirichlet_CIFAR = LeNetDirichletCIFAR()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet_CIFAR.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet_CIFAR.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs, uncertainty_test, _ = lenet_dirichlet_CIFAR(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjuHh7yWWSf6",
        "outputId": "6b7831fb-9af4-49ec-f482-cb37bd244f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 1 - training accuracy: 0.5336 \t training uncertainty: 0.4733 \t testing accuracy: 0.5274 \t testing uncertainty: 0.4389\n",
            "epoch 2 - training accuracy: 0.6552 \t training uncertainty: 0.4384 \t testing accuracy: 0.6229 \t testing uncertainty: 0.3538\n",
            "epoch 3 - training accuracy: 0.7160 \t training uncertainty: 0.3909 \t testing accuracy: 0.6621 \t testing uncertainty: 0.3833\n",
            "epoch 4 - training accuracy: 0.7549 \t training uncertainty: 0.3013 \t testing accuracy: 0.6859 \t testing uncertainty: 0.2759\n",
            "epoch 5 - training accuracy: 0.7840 \t training uncertainty: 0.3863 \t testing accuracy: 0.6945 \t testing uncertainty: 0.2981\n",
            "epoch 6 - training accuracy: 0.8001 \t training uncertainty: 0.2216 \t testing accuracy: 0.6983 \t testing uncertainty: 0.2479\n",
            "epoch 7 - training accuracy: 0.8217 \t training uncertainty: 0.3593 \t testing accuracy: 0.7005 \t testing uncertainty: 0.2493\n",
            "epoch 8 - training accuracy: 0.8373 \t training uncertainty: 0.3225 \t testing accuracy: 0.6983 \t testing uncertainty: 0.3022\n",
            "epoch 9 - training accuracy: 0.8444 \t training uncertainty: 0.2709 \t testing accuracy: 0.7029 \t testing uncertainty: 0.2334\n",
            "epoch 10 - training accuracy: 0.8558 \t training uncertainty: 0.2683 \t testing accuracy: 0.6988 \t testing uncertainty: 0.2128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data1', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data1', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning and regmixup\n",
        "class LeNetDirichletCIFARMixup(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichletCIFARMixup, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8*8*64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Updated regmixup function for Evidential Deep Learning\n",
        "def regmixup(x, y, beta=1.0, cutmix_min=0.0, cutmix_max=0.8):\n",
        "    lambda_ = np.random.beta(beta, beta)\n",
        "    r = np.random.rand(1)\n",
        "    if r < cutmix_min:\n",
        "        return x, y\n",
        "    elif r > cutmix_max:\n",
        "        return x.clone(), y.clone()\n",
        "\n",
        "    # CutMix regularization (optional)\n",
        "    indices = torch.randperm(x.shape[0])  # Shuffle for random mask generation\n",
        "    mask = torch.ones(x.size()).float()\n",
        "    bbx_y = np.random.randint(0, x.size(-2) - int(cutmix_max * x.size(-2)))\n",
        "    bbx_x = np.random.randint(0, x.size(-1) - int(cutmix_max * x.size(-1)))\n",
        "    bbx_y2 = bbx_y + int(cutmix_max * x.size(-2))\n",
        "    bbx_x2 = bbx_x + int(cutmix_max * x.size(-1))\n",
        "    mask[indices, :, bbx_y:bbx_y2, bbx_x:bbx_x2] = 0\n",
        "\n",
        "    mixed_x = lambda_ * x * mask + (1 - lambda_) * x[indices] * (1 - mask)\n",
        "\n",
        "    # Mixed label using weighted average\n",
        "    mixed_y = lambda_ * y + (1 - lambda_) * y[indices]\n",
        "\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = torch.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution and regmixup for Evidential Deep Learning\n",
        "lenet_dirichlet_CIFAR_mixup = LeNetDirichletCIFARMixup()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet_CIFAR_mixup.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet_CIFAR_mixup.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = regmixup(inputs, labels.unsqueeze(1))  # Assuming labels are one-dimensional\n",
        "\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "\n",
        "        # Cast labels back to Long data type\n",
        "        labels = labels.squeeze().long()\n",
        "\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet_CIFAR_mixup.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs, uncertainty_test , _ = lenet_dirichlet_CIFAR_mixup(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ms5UkIxXjHT",
        "outputId": "3355f59d-69a8-4b86-c1e2-cea2f6f34572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data1/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 40890326.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data1/cifar-10-python.tar.gz to ./data1\n",
            "Files already downloaded and verified\n",
            "epoch 1 - training accuracy: 0.4344 \t training uncertainty: 0.7060 \t testing accuracy: 0.4387 \t testing uncertainty: 0.6772\n",
            "epoch 2 - training accuracy: 0.5121 \t training uncertainty: 0.7256 \t testing accuracy: 0.5045 \t testing uncertainty: 0.6903\n",
            "epoch 3 - training accuracy: 0.5276 \t training uncertainty: 0.6137 \t testing accuracy: 0.5162 \t testing uncertainty: 0.5216\n",
            "epoch 4 - training accuracy: 0.5835 \t training uncertainty: 0.7073 \t testing accuracy: 0.5651 \t testing uncertainty: 0.5575\n",
            "epoch 5 - training accuracy: 0.5946 \t training uncertainty: 0.5850 \t testing accuracy: 0.5706 \t testing uncertainty: 0.4733\n",
            "epoch 6 - training accuracy: 0.6307 \t training uncertainty: 0.4513 \t testing accuracy: 0.6012 \t testing uncertainty: 0.4369\n",
            "epoch 7 - training accuracy: 0.6276 \t training uncertainty: 0.5109 \t testing accuracy: 0.5950 \t testing uncertainty: 0.4224\n",
            "epoch 8 - training accuracy: 0.6404 \t training uncertainty: 0.5007 \t testing accuracy: 0.6092 \t testing uncertainty: 0.3916\n",
            "epoch 9 - training accuracy: 0.6496 \t training uncertainty: 0.4447 \t testing accuracy: 0.6130 \t testing uncertainty: 0.4142\n",
            "epoch 10 - training accuracy: 0.6618 \t training uncertainty: 0.4541 \t testing accuracy: 0.6282 \t testing uncertainty: 0.4483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OfXdd5aNMb-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oCHcOIbxLZTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}