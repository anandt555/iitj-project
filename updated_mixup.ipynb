{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandt555/BigData-Practice/blob/main/updated_mixup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRzsU3W1e2P4"
      },
      "source": [
        "**Mixup with MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckE9-VmS53hg",
        "outputId": "1f71bbc0-fab8-43aa-a019-9d0f0b1e204e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training without mixup:\n",
            "epoch 1 - training accuracy: 0.7998\n",
            "epoch 2 - training accuracy: 0.9517\n",
            "epoch 3 - training accuracy: 0.9683\n",
            "epoch 4 - training accuracy: 0.9751\n",
            "epoch 5 - training accuracy: 0.9786\n",
            "epoch 6 - training accuracy: 0.9817\n",
            "epoch 7 - training accuracy: 0.9833\n",
            "epoch 8 - training accuracy: 0.9854\n",
            "epoch 9 - training accuracy: 0.9865\n",
            "epoch 10 - training accuracy: 0.9871\n",
            "Finished Training without mixup\n",
            "Accuracy on test set without mixup: 98.72%\n",
            "\n",
            "Training with mixup:\n",
            "epoch 1 - training accuracy: 0.8869\n",
            "epoch 2 - training accuracy: 0.9655\n",
            "epoch 3 - training accuracy: 0.9737\n",
            "epoch 4 - training accuracy: 0.9764\n",
            "epoch 5 - training accuracy: 0.9828\n",
            "Finished Training with mixup\n",
            "Accuracy on test set with mixup: 98.97%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Define the mixup function for MNIST\n",
        "def mixup_data(x, y, alpha=0.1):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_onehot = torch.zeros(y.size(0), 10)\n",
        "    y_onehot.scatter_(1, y.view(-1, 1), 1)\n",
        "    y[index] = y[index].long()\n",
        "\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# Create LeNet model for MNIST\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Download and prepare MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer for MNIST\n",
        "model_mixup = LeNet()\n",
        "model_no_mixup = LeNet()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_mixup = optim.SGD(model_mixup.parameters(), lr=0.005, momentum=0.9)\n",
        "optimizer_no_mixup = optim.SGD(model_no_mixup.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training and testing loop without mixup\n",
        "print(\"Training without mixup:\")\n",
        "for epoch in range(10):\n",
        "    model_no_mixup.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer_no_mixup.zero_grad()\n",
        "        outputs = model_no_mixup(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_no_mixup.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f' %\n",
        "    (epoch+1, correct / total))\n",
        "        # if i % 200 == 199:  # Print every 200 batches\n",
        "        #     print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 200:.3f} | Accuracy: {100 * correct / total:.2f}%\")\n",
        "        #     running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training without mixup\")\n",
        "\n",
        "# Testing loop without mixup\n",
        "model_no_mixup.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model_no_mixup(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set without mixup: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Training and testing loop with mixup\n",
        "print(\"\\nTraining with mixup:\")\n",
        "for epoch in range(5):\n",
        "    model_mixup.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Apply mixup\n",
        "        inputs, labels = mixup_data(inputs, labels)\n",
        "\n",
        "        optimizer_mixup.zero_grad()\n",
        "        outputs = model_mixup(inputs)\n",
        "        loss = criterion(outputs, torch.argmax(labels, dim=1).long())  # Ensure labels are long type\n",
        "        loss.backward()\n",
        "        optimizer_mixup.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(torch.argmax(labels, dim=1).long()).sum().item()  # Ensure labels are long type\n",
        "    print('epoch %d - training accuracy: %2.4f' %\n",
        "    (epoch+1, correct / total))\n",
        "        # if i % 200 == 199:  # Print every 200 batches\n",
        "        #     print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 200:.3f} | Accuracy: {100 * correct / total:.2f}%\")\n",
        "        #     running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training with mixup\")\n",
        "\n",
        "# Testing loop with mixup\n",
        "model_mixup.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model_mixup(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set with mixup: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcb5KT9Ie_en"
      },
      "source": [
        "**Mixup with CIFAR10**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uCK60oUI54Zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1ed828-04f2-421c-c627-a6ceb2a7d1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training without mixup:\n",
            "epoch 1 - training accuracy: 0.2749\n",
            "epoch 2 - training accuracy: 0.4263\n",
            "epoch 3 - training accuracy: 0.4940\n",
            "epoch 4 - training accuracy: 0.5340\n",
            "epoch 5 - training accuracy: 0.5680\n",
            "epoch 6 - training accuracy: 0.5982\n",
            "epoch 7 - training accuracy: 0.6248\n",
            "epoch 8 - training accuracy: 0.6478\n",
            "epoch 9 - training accuracy: 0.6680\n",
            "epoch 10 - training accuracy: 0.6868\n",
            "Finished Training without mixup\n",
            "Accuracy on test set without mixup: 66.13%\n",
            "\n",
            "Training with mixup:\n",
            "epoch 1 - training accuracy: 0.3906\n",
            "epoch 2 - training accuracy: 0.5504\n",
            "epoch 3 - training accuracy: 0.6287\n",
            "epoch 4 - training accuracy: 0.6863\n",
            "epoch 5 - training accuracy: 0.7274\n",
            "Finished Training with mixup\n",
            "Accuracy on test set with mixup: 71.29%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Define the mixup function for CIFAR-10\n",
        "def mixup_data(x, y, alpha=0.1):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_onehot = torch.zeros(y.size(0), 10)\n",
        "    y_onehot.scatter_(1, y.view(-1, 1), 1)\n",
        "    y[index] = y[index].long()\n",
        "\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# Create Adjusted LeNet model for CIFAR-10\n",
        "class AdjustedLeNetLike(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdjustedLeNetLike, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)  # Adjust input channels\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 64, 512)  # Adjust the input size for fully connected layer\n",
        "        self.fc2 = nn.Linear(512, 10)  # Adjust output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 8 * 8 * 64)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Download and prepare CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer for CIFAR-10\n",
        "model_mixup = AdjustedLeNetLike()\n",
        "model_no_mixup = AdjustedLeNetLike()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_mixup = optim.SGD(model_mixup.parameters(), lr=0.005, momentum=0.9)\n",
        "optimizer_no_mixup = optim.SGD(model_no_mixup.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training and testing loop without mixup\n",
        "print(\"Training without mixup:\")\n",
        "for epoch in range(10):\n",
        "    model_no_mixup.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer_no_mixup.zero_grad()\n",
        "        outputs = model_no_mixup(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_no_mixup.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    print('epoch %d - training accuracy: %2.4f' %\n",
        "    (epoch+1, correct / total))\n",
        "\n",
        "print(\"Finished Training without mixup\")\n",
        "\n",
        "# Testing loop without mixup\n",
        "model_no_mixup.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model_no_mixup(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set without mixup: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Training and testing loop with mixup\n",
        "print(\"\\nTraining with mixup:\")\n",
        "for epoch in range(5):\n",
        "    model_mixup.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Apply mixup\n",
        "        inputs, labels = mixup_data(inputs, labels)\n",
        "\n",
        "        optimizer_mixup.zero_grad()\n",
        "        outputs = model_mixup(inputs)\n",
        "        loss = criterion(outputs, torch.argmax(labels, dim=1).long())  # Ensure labels are long type\n",
        "        loss.backward()\n",
        "        optimizer_mixup.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(torch.argmax(labels, dim=1).long()).sum().item()  # Ensure labels are long type\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f' %\n",
        "    (epoch+1, correct / total))\n",
        "\n",
        "print(\"Finished Training with mixup\")\n",
        "\n",
        "# Testing loop with mixup\n",
        "model_mixup.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model_mixup(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set with mixup: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBnn4hxEWhsR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmW7hq2JD9Fi5iSIN9fF0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}