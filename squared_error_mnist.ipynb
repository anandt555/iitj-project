{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandt555/BigData-Practice/blob/main/squared_error_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDL with mnist with Custom loss(squared error + KL Divergence Term)"
      ],
      "metadata": {
        "id": "dWGJXjRMLzsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4j-vLLH6aNV",
        "outputId": "d5a67d5a-b461-4ecd-a076-afc8592598b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 - training accuracy: 0.9348 \t training uncertainty: 0.4443 \t testing accuracy: 0.9426 \t testing uncertainty: 0.4405\n",
            "epoch 2 - training accuracy: 0.9641 \t training uncertainty: 0.2648 \t testing accuracy: 0.9678 \t testing uncertainty: 0.2562\n",
            "epoch 3 - training accuracy: 0.9740 \t training uncertainty: 0.2004 \t testing accuracy: 0.9765 \t testing uncertainty: 0.1908\n",
            "epoch 4 - training accuracy: 0.9799 \t training uncertainty: 0.1740 \t testing accuracy: 0.9809 \t testing uncertainty: 0.1600\n",
            "epoch 5 - training accuracy: 0.9833 \t training uncertainty: 0.1423 \t testing accuracy: 0.9828 \t testing uncertainty: 0.1336\n",
            "epoch 6 - training accuracy: 0.9858 \t training uncertainty: 0.1347 \t testing accuracy: 0.9853 \t testing uncertainty: 0.1195\n",
            "epoch 7 - training accuracy: 0.9884 \t training uncertainty: 0.1159 \t testing accuracy: 0.9873 \t testing uncertainty: 0.1086\n",
            "epoch 8 - training accuracy: 0.9895 \t training uncertainty: 0.1127 \t testing accuracy: 0.9882 \t testing uncertainty: 0.0980\n",
            "epoch 9 - training accuracy: 0.9910 \t training uncertainty: 0.0970 \t testing accuracy: 0.9893 \t testing uncertainty: 0.0899\n",
            "epoch 10 - training accuracy: 0.9917 \t training uncertainty: 0.0966 \t testing accuracy: 0.9895 \t testing uncertainty: 0.0806\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Download MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = F.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichlet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichlet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1  # Use absolute values for simplicity, adjust as needed\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "lenet_dirichlet = LeNetDirichlet()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet(inputs)\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train, _ = lenet_dirichlet(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs, uncertainty_test, _ = lenet_dirichlet(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDL + mixup with mnist with Custom loss(squared error + KL Divergence Term)"
      ],
      "metadata": {
        "id": "H7OtfjS0HqzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8lT3COqIVZL",
        "outputId": "0d932c41-cdc2-4658-bf5b-b976a200a385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 - training accuracy: 0.9460 \t training uncertainty: 0.4117 \t testing accuracy: 0.9525 \t testing uncertainty: 0.4051\n",
            "epoch 2 - training accuracy: 0.9666 \t training uncertainty: 0.2740 \t testing accuracy: 0.9708 \t testing uncertainty: 0.2641\n",
            "epoch 3 - training accuracy: 0.9747 \t training uncertainty: 0.2210 \t testing accuracy: 0.9781 \t testing uncertainty: 0.2047\n",
            "epoch 4 - training accuracy: 0.9795 \t training uncertainty: 0.1997 \t testing accuracy: 0.9813 \t testing uncertainty: 0.1913\n",
            "epoch 5 - training accuracy: 0.9824 \t training uncertainty: 0.1661 \t testing accuracy: 0.9843 \t testing uncertainty: 0.1552\n",
            "epoch 6 - training accuracy: 0.9838 \t training uncertainty: 0.1484 \t testing accuracy: 0.9857 \t testing uncertainty: 0.1378\n",
            "epoch 7 - training accuracy: 0.9861 \t training uncertainty: 0.1580 \t testing accuracy: 0.9875 \t testing uncertainty: 0.1494\n",
            "epoch 8 - training accuracy: 0.9871 \t training uncertainty: 0.1320 \t testing accuracy: 0.9876 \t testing uncertainty: 0.1237\n",
            "epoch 9 - training accuracy: 0.9881 \t training uncertainty: 0.1289 \t testing accuracy: 0.9891 \t testing uncertainty: 0.1184\n",
            "epoch 10 - training accuracy: 0.9881 \t training uncertainty: 0.1123 \t testing accuracy: 0.9886 \t testing uncertainty: 0.1030\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Download MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = F.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "# Mixup function for Evidential Deep Learning\n",
        "def mixup_data(x, y, alpha=0.1):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_onehot = torch.zeros(y.size(0), 10).to(x.device)\n",
        "    y_onehot.scatter_(1, y.view(-1, 1).long(), 1)\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "    mixed_y = mixed_y.argmax(dim=1)\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichlet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichlet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1  # Use absolute values for simplicity, adjust as needed\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "lenet_dirichlet = LeNetDirichlet()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = mixup_data(inputs, labels.unsqueeze(1).float())  # Applying mixup\n",
        "\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet(inputs)\n",
        "        loss = custom_loss(F.one_hot(labels, num_classes=10).float(), outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train, _ = lenet_dirichlet(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs, uncertainty_test, _ = lenet_dirichlet(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDL + regmixup with mnist with Custom loss(squared error + KL Divergence Term)"
      ],
      "metadata": {
        "id": "DMrpTlCBLfgf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqGor8yvTttI",
        "outputId": "93d33ff7-18e5-4e59-9b87-fac4be403320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 - training accuracy: 0.9070 \t training uncertainty: 0.5979 \t testing accuracy: 0.9108 \t testing uncertainty: 0.5910\n",
            "epoch 2 - training accuracy: 0.9539 \t training uncertainty: 0.5339 \t testing accuracy: 0.9579 \t testing uncertainty: 0.5261\n",
            "epoch 3 - training accuracy: 0.9650 \t training uncertainty: 0.4187 \t testing accuracy: 0.9685 \t testing uncertainty: 0.4055\n",
            "epoch 4 - training accuracy: 0.9710 \t training uncertainty: 0.3606 \t testing accuracy: 0.9733 \t testing uncertainty: 0.3449\n",
            "epoch 5 - training accuracy: 0.9758 \t training uncertainty: 0.3616 \t testing accuracy: 0.9780 \t testing uncertainty: 0.3441\n",
            "epoch 6 - training accuracy: 0.9786 \t training uncertainty: 0.3318 \t testing accuracy: 0.9810 \t testing uncertainty: 0.3166\n",
            "epoch 7 - training accuracy: 0.9815 \t training uncertainty: 0.3298 \t testing accuracy: 0.9841 \t testing uncertainty: 0.3139\n",
            "epoch 8 - training accuracy: 0.9831 \t training uncertainty: 0.3069 \t testing accuracy: 0.9851 \t testing uncertainty: 0.2851\n",
            "epoch 9 - training accuracy: 0.9841 \t training uncertainty: 0.2704 \t testing accuracy: 0.9856 \t testing uncertainty: 0.2567\n",
            "epoch 10 - training accuracy: 0.9847 \t training uncertainty: 0.2631 \t testing accuracy: 0.9861 \t testing uncertainty: 0.2485\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Download MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define custom loss function and KL divergence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def KL(alpha, num_classes=10):\n",
        "    one = torch.ones((1, num_classes), dtype=torch.float32)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True) + \\\n",
        "         torch.sum(torch.lgamma(one), dim=1, keepdim=True) - torch.lgamma(torch.sum(one, dim=1, keepdim=True)) + \\\n",
        "         torch.sum((alpha - one) * (torch.digamma(alpha) - torch.digamma(S)), dim=1, keepdim=True)\n",
        "\n",
        "    return kl\n",
        "\n",
        "def custom_loss(y_true, output):\n",
        "    epochs = [1]\n",
        "\n",
        "    y_evidence = F.relu(output)\n",
        "    alpha = y_evidence + 1\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    p = alpha / S\n",
        "\n",
        "    err = torch.sum(torch.pow((y_true - p), 2), dim=1, keepdim=True)\n",
        "    var = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)\n",
        "\n",
        "    l = torch.sum(err + var, dim=1, keepdim=True)\n",
        "\n",
        "    kl = torch.min(torch.tensor(1.0), torch.tensor(epochs[0] / 50)) * torch.sum(KL((1 - y_true) * (alpha) + y_true))\n",
        "    return torch.sum(l + kl)\n",
        "\n",
        "\n",
        "def regmixup(x, y, beta=1.0, cutmix_min=0.0, cutmix_max=0.8):\n",
        "    lambda_ = np.random.beta(beta, beta)\n",
        "    r = np.random.rand(1)\n",
        "    if r < cutmix_min:\n",
        "        return x, y\n",
        "    elif r > cutmix_max:\n",
        "        return x.clone(), y.clone()\n",
        "\n",
        "    # CutMix regularization (optional)\n",
        "    indices = torch.randperm(x.shape[0])  # Shuffle for random mask generation\n",
        "    mask = torch.ones(x.size()).float()\n",
        "    bbx_y = np.random.randint(0, x.size(-2) - int(cutmix_max * x.size(-2)))\n",
        "    bbx_x = np.random.randint(0, x.size(-1) - int(cutmix_max * x.size(-1)))\n",
        "    bbx_y2 = bbx_y + int(cutmix_max * x.size(-2))\n",
        "    bbx_x2 = bbx_x + int(cutmix_max * x.size(-1))\n",
        "    mask[indices, :, bbx_y:bbx_y2, bbx_x:bbx_x2] = 0\n",
        "\n",
        "    mixed_x = lambda_ * x * mask + (1 - lambda_) * x[indices] * (1 - mask)\n",
        "\n",
        "    # Mixed label using weighted average\n",
        "    mixed_y = lambda_ * y + (1 - lambda_) * y[indices]\n",
        "\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "\n",
        "# LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "class LeNetDirichlet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDirichlet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        alpha = torch.abs(x) + 1  # Use absolute values for simplicity, adjust as needed\n",
        "\n",
        "        u = 10 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "        prob = alpha / torch.sum(alpha, 1, keepdim=True)\n",
        "\n",
        "        return prob, u, alpha\n",
        "\n",
        "# Train LeNet model with Dirichlet distribution for Evidential Deep Learning\n",
        "lenet_dirichlet = LeNetDirichlet()\n",
        "optimizer_dirichlet = optim.Adam(lenet_dirichlet.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    lenet_dirichlet.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        labels_onehot = F.one_hot(labels, num_classes=10).float()\n",
        "        inputs, labels_onehot = regmixup(inputs, labels_onehot)  # Applying mixup\n",
        "\n",
        "        optimizer_dirichlet.zero_grad()\n",
        "        outputs, _, _ = lenet_dirichlet(inputs)\n",
        "        loss = custom_loss(labels_onehot, outputs)  # Using custom loss\n",
        "        loss.backward()\n",
        "        optimizer_dirichlet.step()\n",
        "\n",
        "    lenet_dirichlet.eval()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_loader:\n",
        "            outputs, uncertainty_train, _ = lenet_dirichlet(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()  # Fix dimension here\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs, uncertainty_test, _ = lenet_dirichlet(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()  # Fix dimension here\n",
        "\n",
        "    print('epoch %d - training accuracy: %2.4f \\t training uncertainty: %2.4f \\t testing accuracy: %2.4f \\t testing uncertainty: %2.4f' %\n",
        "          (epoch+1, correct_train / total_train, uncertainty_train.mean(), correct_test / total_test, uncertainty_test.mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33jBdz8AYdDi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMx+y1xGOAgzdukMA79F+j0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}